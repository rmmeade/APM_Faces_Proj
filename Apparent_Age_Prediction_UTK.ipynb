{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get images from UTKFace at https://susanqq.github.io/UTKFace/\n",
    "## In-the-wild version was used for these models\n",
    "## Based on code found on https://github.com/serengil/tensorflow-101\n",
    "\n",
    "#os.chdir(r'C:\\Users\\user\\') # change it to the path that contains table(mat).csv\n",
    "df = pd.read_csv('utk_uncropped_all/utkCrop_no16.csv', usecols=['age', 'gender', 'race', 'date&time', 'filename'])\n",
    "df.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>date&amp;time</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112213500903</td>\n",
       "      <td>100_0_0_20170112213500903.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112215240346</td>\n",
       "      <td>100_0_0_20170112215240346.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110183726390</td>\n",
       "      <td>100_1_0_20170110183726390.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112213001988</td>\n",
       "      <td>100_1_0_20170112213001988.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112213303693</td>\n",
       "      <td>100_1_0_20170112213303693.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  race          date&time                       filename\n",
       "0  100     0.0     0  20170112213500903  100_0_0_20170112213500903.jpg\n",
       "1  100     0.0     0  20170112215240346  100_0_0_20170112215240346.jpg\n",
       "2  100     1.0     0  20170110183726390  100_1_0_20170110183726390.jpg\n",
       "3  100     1.0     0  20170112213001988  100_1_0_20170112213001988.jpg\n",
       "4  100     1.0     0  20170112213303693  100_1_0_20170112213303693.jpg"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'date&time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112213500903.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112215240346.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170110183726390.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213001988.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213303693.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  race                       filename\n",
       "0  100     0.0     0  100_0_0_20170112213500903.jpg\n",
       "1  100     0.0     0  100_0_0_20170112215240346.jpg\n",
       "2  100     1.0     0  100_1_0_20170110183726390.jpg\n",
       "3  100     1.0     0  100_1_0_20170112213001988.jpg\n",
       "4  100     1.0     0  100_1_0_20170112213303693.jpg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some guys seem to be greater than 100. some of these are paintings. remove these old guys\n",
    "df = df[df['age'] <= 100]\n",
    "\n",
    "#some guys seem to be unborn in the data set\n",
    "df = df[df['age'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram = df['age'].hist(bins=df['age'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output classes:  101\n"
     ]
    }
   ],
   "source": [
    "classes = 101 #(0, 100])\n",
    "print(\"number of output classes: \",classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "\n",
    "def getImagePixels(image_path):\n",
    "    img = image.load_img(\"utk_uncropped_all/%s\" % image_path, grayscale=False, target_size=target_size)\n",
    "    x = image.img_to_array(img).reshape(1, -1)[0]\n",
    "    #x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pixels'] = df['filename'].apply(getImagePixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>filename</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112213500903.jpg</td>\n",
       "      <td>[51.0, 55.0, 64.0, 82.0, 86.0, 95.0, 73.0, 77....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112215240346.jpg</td>\n",
       "      <td>[113.0, 117.0, 126.0, 113.0, 117.0, 126.0, 114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170110183726390.jpg</td>\n",
       "      <td>[226.0, 229.0, 236.0, 232.0, 235.0, 242.0, 227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213001988.jpg</td>\n",
       "      <td>[65.0, 72.0, 80.0, 65.0, 72.0, 80.0, 65.0, 72....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213303693.jpg</td>\n",
       "      <td>[31.0, 36.0, 40.0, 31.0, 36.0, 40.0, 31.0, 36....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  race                       filename  \\\n",
       "0  100     0.0     0  100_0_0_20170112213500903.jpg   \n",
       "1  100     0.0     0  100_0_0_20170112215240346.jpg   \n",
       "2  100     1.0     0  100_1_0_20170110183726390.jpg   \n",
       "3  100     1.0     0  100_1_0_20170112213001988.jpg   \n",
       "4  100     1.0     0  100_1_0_20170112213303693.jpg   \n",
       "\n",
       "                                              pixels  \n",
       "0  [51.0, 55.0, 64.0, 82.0, 86.0, 95.0, 73.0, 77....  \n",
       "1  [113.0, 117.0, 126.0, 113.0, 117.0, 126.0, 114...  \n",
       "2  [226.0, 229.0, 236.0, 232.0, 235.0, 242.0, 227...  \n",
       "3  [65.0, 72.0, 80.0, 65.0, 72.0, 80.0, 65.0, 72....  \n",
       "4  [31.0, 36.0, 40.0, 31.0, 36.0, 40.0, 31.0, 36....  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['age'].values\n",
    "target_classes = keras.utils.to_categorical(target, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x, ts_x, train_y, test_y = train_test_split(df, target_classes\n",
    "                                        , test_size=0.30, random_state=42)#, stratify=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use these for duplicating or triplicating >= 60 (balancing the data)\n",
    "#tr_60 = tr_x[tr_x['age'] >= 60]\n",
    "#tr_y60 = train_y[tr_x['age'] >= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use these for duplicating \n",
    "# tr_x = tr_x.append(tr_60)\n",
    "# train_y = np.concatenate((train_y, tr_y60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use these for triplicating > 60\n",
    "# tr_x = tr_x.append(tr_60)\n",
    "# tr_x = tr_x.append(tr_60)\n",
    "# train_y = np.concatenate((train_y, tr_y60))\n",
    "# train_y = np.concatenate((train_y, tr_y60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use these for duplicating or triplicating\n",
    "#histogram = tr_x['age'].hist(bins=tr_x['age'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = df['pixels'].values\n",
    "features = []\n",
    "\n",
    "for i in range(0, tr_x.shape[0]):\n",
    "    features.append(tr_x['pixels'].values[i])\n",
    "\n",
    "features = np.array(features)\n",
    "features = features.reshape(features.shape[0], 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = df['pixels'].values\n",
    "features_test = []\n",
    "\n",
    "for i in range(0, ts_x.shape[0]):\n",
    "    features_test.append(ts_x['pixels'].values[i])\n",
    "\n",
    "features_test = np.array(features_test)\n",
    "features_test = features_test.reshape(features_test.shape[0], 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13799, 224, 224, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5915, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features /= 255 #normalize in [0, 1]\n",
    "features_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features\n",
    "test_x = features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-Face model\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-trained weights of vgg-face model. \n",
    "#you can find it here: https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "#related blog post: https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze all layers of VGG-Face except last 7\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model_output = Sequential()\n",
    "base_model_output = Convolution2D(classes, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "base_model_output = Flatten()(base_model_output)\n",
    "base_model_output = Activation('softmax')(base_model_output)\n",
    "\n",
    "age_model = Model(inputs=model.input, outputs=base_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check trainable layers\n",
    "if False:\n",
    "    for layer in model.layers:\n",
    "        print(layer, layer.trainable)\n",
    "\n",
    "    print(\"------------------------\")\n",
    "    for layer in age_model.layers:\n",
    "        print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose between Adam and sgd optimizer, usually Adam converges after 30 epochs, Sgd over 250 still not converging.\n",
    "sgd = keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "age_model.compile(loss='categorical_crossentropy'\n",
    "                  , optimizer=keras.optimizers.Adam()\n",
    "                  #, optimizer = sgd\n",
    "                  , metrics=['accuracy']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='classification_age_model_utk.hdf5'\n",
    "    , monitor = \"val_loss\"\n",
    "    , verbose=1\n",
    "    , save_best_only=True\n",
    "    , mode = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True retrains the NN\n",
    "enableFit = False\n",
    "\n",
    "if enableFit:\n",
    "    epochs = 100\n",
    "    batch_size = 512\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(\"epoch \",i)\n",
    "        \n",
    "        ix_train = np.random.choice(train_x.shape[0], size=batch_size)\n",
    "        \n",
    "        score = age_model.fit(\n",
    "            train_x[ix_train], train_y[ix_train]\n",
    "            , epochs=1\n",
    "            , validation_data=(test_x, test_y)\n",
    "            , callbacks=[checkpointer]\n",
    "        )\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    #restore the best weights\n",
    "    from keras.models import load_model\n",
    "    age_model = load_model(\"classification_age_model_utk.hdf5\")\n",
    "    \n",
    "    age_model.save_weights('age_model_weights_utk.h5')\n",
    "        \n",
    "else:\n",
    "    ##Rename accordingly if needed:\n",
    "    #pre-trained weights for age prediction (Wiki): https://drive.google.com/file/d/1YCox_4kJ-BYeXq27uUbasu--yz28zUMV/view?usp=sharing\n",
    "    #pre-trained weights for age prediction (UTK):https://drive.google.com/file/d/10-S6IIkahO4f1BBsVPPcB8Z4V4tepb6c/view?usp=sharing\n",
    "    #pre-trained weights for age prediction with 2x 60+ people: https://drive.google.com/file/d/10-0W2subbpH_ZmUopdEAt9barSEWyyQX/view?usp=sharing\n",
    "    #pre-trained weights for age prediction with 3x 60+ people:https://drive.google.com/file/d/1hp6uy01YKu-USMo_ef60MtCNL8bJUlP-/view?usp=sharing\n",
    "    age_model.load_weights(\"age_model_weights_utk.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only prints if model is trained\n",
    "val_loss_change = []; loss_change = []\n",
    "for i in range(0, len(scores)):\n",
    "    val_loss_change.append(scores[i].history['val_loss'])\n",
    "    loss_change.append(scores[i].history['loss'])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "plt.plot(val_loss_change, label='val_loss')\n",
    "plt.plot(loss_change, label='train_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5915/5915 [==============================] - 48s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.502568178829861, 0.12071005917537646]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss and accuracy on validation set\n",
    "age_model.evaluate(test_x, test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = age_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indexes = np.array([i for i in range(0, 101)])\n",
    "apparent_predictions = np.sum(predictions * output_indexes, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\.conda\\envs\\jerry\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\sebas\\.conda\\envs\\jerry\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ts_x['Weighted_Avg'] = apparent_predictions\n",
    "argmax = []\n",
    "for p in predictions: \n",
    "    predm = np.argmax(p)\n",
    "    argmax.append(predm)\n",
    "ts_x['ArgMax'] = argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>filename</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Weighted_Avg</th>\n",
       "      <th>ArgMax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4531</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>26_0_4_20170117153320853.jpg</td>\n",
       "      <td>[156.0, 161.0, 167.0, 156.0, 161.0, 167.0, 156...</td>\n",
       "      <td>29.530177</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40_0_0_20170117133805950.jpg</td>\n",
       "      <td>[206.0, 206.0, 206.0, 206.0, 206.0, 206.0, 206...</td>\n",
       "      <td>39.752235</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19_0_2_20170116212031170.jpg</td>\n",
       "      <td>[255.0, 255.0, 255.0, 255.0, 255.0, 255.0, 255...</td>\n",
       "      <td>30.157920</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7561</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>29_0_2_20170116172141170.jpg</td>\n",
       "      <td>[250.0, 251.0, 253.0, 250.0, 251.0, 253.0, 250...</td>\n",
       "      <td>27.181044</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26_1_1_20170116153419888.jpg</td>\n",
       "      <td>[136.0, 138.0, 135.0, 136.0, 138.0, 135.0, 136...</td>\n",
       "      <td>27.079226</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5104</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26_1_1_20170116231832606.jpg</td>\n",
       "      <td>[175.0, 182.0, 190.0, 174.0, 181.0, 189.0, 174...</td>\n",
       "      <td>30.061464</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11872</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>37_1_4_20170117203627007.jpg</td>\n",
       "      <td>[199.0, 167.0, 120.0, 199.0, 167.0, 120.0, 199...</td>\n",
       "      <td>32.829794</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10843</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35_1_0_20170117152140763.jpg</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>29.983485</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5747</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>26_1_4_20170117143809650.jpg</td>\n",
       "      <td>[200.0, 140.0, 116.0, 211.0, 153.0, 129.0, 209...</td>\n",
       "      <td>24.077150</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13208</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42_0_0_20170119204427263.jpg</td>\n",
       "      <td>[111.0, 91.0, 84.0, 112.0, 92.0, 85.0, 113.0, ...</td>\n",
       "      <td>45.955700</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5915 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  gender  race                      filename  \\\n",
       "4531    26     0.0     4  26_0_4_20170117153320853.jpg   \n",
       "12550   40     0.0     0  40_0_0_20170117133805950.jpg   \n",
       "488     19     0.0     2  19_0_2_20170116212031170.jpg   \n",
       "7561    29     0.0     2  29_0_2_20170116172141170.jpg   \n",
       "5000    26     1.0     1  26_1_1_20170116153419888.jpg   \n",
       "...    ...     ...   ...                           ...   \n",
       "5104    26     1.0     1  26_1_1_20170116231832606.jpg   \n",
       "11872   37     1.0     4  37_1_4_20170117203627007.jpg   \n",
       "10843   35     1.0     0  35_1_0_20170117152140763.jpg   \n",
       "5747    26     1.0     4  26_1_4_20170117143809650.jpg   \n",
       "13208   42     0.0     0  42_0_0_20170119204427263.jpg   \n",
       "\n",
       "                                                  pixels  Weighted_Avg  ArgMax  \n",
       "4531   [156.0, 161.0, 167.0, 156.0, 161.0, 167.0, 156...     29.530177      26  \n",
       "12550  [206.0, 206.0, 206.0, 206.0, 206.0, 206.0, 206...     39.752235      40  \n",
       "488    [255.0, 255.0, 255.0, 255.0, 255.0, 255.0, 255...     30.157920      22  \n",
       "7561   [250.0, 251.0, 253.0, 250.0, 251.0, 253.0, 250...     27.181044      23  \n",
       "5000   [136.0, 138.0, 135.0, 136.0, 138.0, 135.0, 136...     27.079226      26  \n",
       "...                                                  ...           ...     ...  \n",
       "5104   [175.0, 182.0, 190.0, 174.0, 181.0, 189.0, 174...     30.061464      22  \n",
       "11872  [199.0, 167.0, 120.0, 199.0, 167.0, 120.0, 199...     32.829794      26  \n",
       "10843  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     29.983485      23  \n",
       "5747   [200.0, 140.0, 116.0, 211.0, 153.0, 129.0, 209...     24.077150      23  \n",
       "13208  [111.0, 91.0, 84.0, 112.0, 92.0, 85.0, 113.0, ...     45.955700      37  \n",
       "\n",
       "[5915 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  7.7565511411665256\n",
      "instances:  5915\n"
     ]
    }
   ],
   "source": [
    "mae = 0\n",
    "\n",
    "for i in range(0 ,apparent_predictions.shape[0]):\n",
    "    prediction = int(apparent_predictions[i])\n",
    "    actual = np.argmax(test_y[i])\n",
    "    \n",
    "    abs_error = abs(prediction - actual)\n",
    "#     actual_mean = actual_mean + actual\n",
    "    \n",
    "    mae = mae + abs_error\n",
    "    \n",
    "mae = mae / apparent_predictions.shape[0]\n",
    "\n",
    "print(\"mae: \",mae)\n",
    "print(\"instances: \",apparent_predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the output according to your code\n",
    "#ts_x.to_csv('utk_uncropped_retrained_age_adam_with_dup.csv')\n",
    "#ts_x.to_csv('utk_uncropped_retrained_age_adam_with_dup_x3.csv')\n",
    "#ts_x.to_csv('error_wiki_test_utk_adam.csv')\n",
    "#ts_x.to_csv('utk_uncropped_retrained_age_adam_with_dup_x3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Measurement for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = age_model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indexes = np.array([i for i in range(0, 101)])\n",
    "apparent_predictions = np.sum(predictions * output_indexes, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  6.9188319975496455\n",
      "instances:  19589\n"
     ]
    }
   ],
   "source": [
    "mae = 0\n",
    "\n",
    "for i in range(0 ,apparent_predictions.shape[0]):\n",
    "    prediction = int(apparent_predictions[i])\n",
    "    actual = np.argmax(train_y[i])\n",
    "    \n",
    "    abs_error = abs(prediction - actual)\n",
    "#     actual_mean = actual_mean + actual\n",
    "    \n",
    "    mae = mae + abs_error\n",
    "    \n",
    "mae = mae / apparent_predictions.shape[0]\n",
    "\n",
    "print(\"mae: \",mae)\n",
    "print(\"instances: \",apparent_predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model on a custom image\n",
    "\n",
    "Feed an image to find the apparent age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(filepath):\n",
    "    test_img = image.load_img(filepath, target_size=(224, 224))\n",
    "    test_img = image.img_to_array(test_img)\n",
    "    test_img = np.expand_dims(test_img, axis = 0)\n",
    "    test_img /= 255\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your own pictures to test\n",
    "picture = \"S1.jpg\"\n",
    "prediction = age_model.predict(loadImage(picture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(101)\n",
    "plt.bar(y_pos, prediction[0], align='center', alpha=0.3)\n",
    "plt.ylabel('percentage')\n",
    "plt.title('age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(picture)#, target_size=(224, 224))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "print(\"most dominant age class (not apparent age): \",np.argmax(prediction))\n",
    "\n",
    "apparent_age = np.round(np.sum(prediction * output_indexes, axis = 1))\n",
    "print(\"apparent age: \", int(apparent_age[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
